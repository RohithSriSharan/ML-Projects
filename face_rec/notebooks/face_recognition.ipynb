{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03249504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, io, torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1462c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aishwarya Rai', 'Hrithik Roshan', 'NTR']\n"
     ]
    }
   ],
   "source": [
    "ROOT = os.path.abspath(os.path.join(os.getcwd(),\"..\"))\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT, \"data\")\n",
    "\n",
    "CLASSES = [c for c in os.listdir(DATA_DIR)]\n",
    "\n",
    "print(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2e2ddad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "{'Aishwarya Rai': 6, 'Hrithik Roshan': 6, 'NTR': 6}\n"
     ]
    }
   ],
   "source": [
    "Total_images = 0\n",
    "class_counts = {}\n",
    "for cls in CLASSES:\n",
    "    cls_path = os.path.join(DATA_DIR, cls)\n",
    "    count = 0\n",
    "    for f in os.listdir(cls_path):\n",
    "        Total_images += 1\n",
    "        count += 1\n",
    "    class_counts[cls] = count\n",
    "print(Total_images)\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58830b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded: ViTModel\n",
      "Embedding size: 768\n"
     ]
    }
   ],
   "source": [
    "# pretrained Vision Transformer (ViT)\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model = AutoModel.from_pretrained(\"google/vit-base-patch16-224\").eval()\n",
    "\n",
    "print(\"✅ Model loaded:\", model.__class__.__name__)\n",
    "print(\"Embedding size:\", model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0df306cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: c:\\Users\\HP\\Desktop\\Python_code_practice\\ml_projects\\face_rec\\data\\Aishwarya Rai\\1.jpg\n",
      "Embedding shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# pick first image of first class\n",
    "sample_img = os.path.join(DATA_DIR, classes[0], os.listdir(os.path.join(DATA_DIR, classes[0]))[0])\n",
    "print(\"Testing:\", sample_img)\n",
    "\n",
    "img = Image.open(sample_img).convert(\"RGB\")\n",
    "inputs = processor(images=img, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "emb = outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "print(\"Embedding shape:\", emb.shape)  # should be (1, 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e1163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
